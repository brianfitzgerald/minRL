"""
this script requires vllm's v0 engine which has a hacky way to reload model weights
so we have pinned to v0.10.2 release of vllm

uv run rl_full.py --lr 1e-5 --disable_wandb
"""

import argparse
import os
import random
import time

import torch
from datasets import load_dataset
from math_utils import is_equiv, last_boxed_only_string, remove_boxed
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizerBase
from vllm import LLM, SamplingParams

import wandb

os.environ["VLLM_USE_V1"] = (
    "0"  # use v0 engine because it lets us move weights between hf_model and vllm_model
)


def parse_args():
    parser = argparse.ArgumentParser(description="Train a language model with GRPO")

    # Model configuration
    parser.add_argument(
        "--model_id",
        type=str,
        default="Qwen/Qwen3-1.7B",
        help="HuggingFace model ID to use",
    )

    parser.add_argument("--lr", type=float, default=1e-5, help="Learning rate")
    parser.add_argument(
        "--n_grpo_steps",
        type=int,
        default=50,
        help="Number of GRPO training steps",
    )
    parser.add_argument(
        "--n_prompts_per_step",
        type=int,
        default=32,
        help="Number of prompts to sample per step",
    )
    parser.add_argument(
        "--group_size",
        type=int,
        default=8,
        help="Number of rollouts per prompt",
    )
    parser.add_argument(
        "--epochs_per_step",
        type=int,
        default=1,
        help="Number of epochs to train per step",
    )
    parser.add_argument(
        "--micro_batch_size",
        type=int,
        default=4,
        help="Micro batch size for training",
    )
    parser.add_argument(
        "--gradient_accumulation_steps",
        type=int,
        default=64,
        help="Number of gradient accumulation steps",
    )

    # Other configuration
    parser.add_argument(
        "--base_dir",
        type=str,
        default="runs",
        help="Base directory for saving runs",
    )
    parser.add_argument(
        "--prompt_template",
        type=str,
        default="boxed.prompt",
        help="Path to prompt template file",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed for reproducibility",
    )

    # Wandb configuration
    parser.add_argument(
        "--wandb_project",
        type=str,
        default="math_grpo_full",
        help="Wandb project name",
    )
    parser.add_argument(
        "--wandb_run_name",
        type=str,
        default=None,
        help="Wandb run name (defaults to run directory name)",
    )
    parser.add_argument(
        "--disable_wandb",
        action="store_true",
        help="Disable wandb logging",
    )

    return parser.parse_args()


def main():
    args = parse_args()

    # Set random seed
    random.seed(args.seed)
    torch.manual_seed(args.seed)

    print(f"Training configuration:")
    print(f"  Model ID: {args.model_id}")
    print(f"  Learning rate: {args.lr}")
    print(f"  W&B logging: {'enabled' if not args.disable_wandb else 'disabled'}")
    print()

    # Setup run directory
    os.makedirs(args.base_dir, exist_ok=True)
    i = 1
    while os.path.exists(f"{args.base_dir}/{i}"):
        i += 1
    run_name = f"{args.base_dir}/{i}"
    os.makedirs(run_name)
    print(f"Created: {run_name}")

    # Initialize wandb
    if not args.disable_wandb:
        wandb_run_name = args.wandb_run_name or f"{args.model_id}_{args.lr:.1e}_full"
        wandb.init(
            project=args.wandb_project,
            name=wandb_run_name,
            config=vars(args),
            dir=run_name,
        )
        # Log the run directory
        wandb.config.update({"run_dir": run_name})

    # Load dataset and tokenizer
    train_dataset = load_dataset("qwedsacf/competition_math", split=f"train[:7500]")
    val_dataset = load_dataset("qwedsacf/competition_math", split=f"train[-5000:]")
    tokenizer = AutoTokenizer.from_pretrained(args.model_id)
    if tokenizer.pad_token_id == None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    # Load prompt template
    with open(args.prompt_template, "r", encoding="utf-8") as f:
        template = f.read().strip()

    def process_data(example):
        with_template = template.replace("{question}", example["problem"])
        prompt = tokenizer.apply_chat_template(
            [{"role": "user", "content": with_template}],
            tokenize=False,
            add_generation_prompt=True,
        )
        answer = remove_boxed(last_boxed_only_string(example["solution"]))
        return {"prompt": prompt, "answer": answer}

    train_dataset = train_dataset.map(process_data)
    val_dataset = val_dataset.map(process_data)

    vllm_model = LLM(
        model=args.model_id,
        tensor_parallel_size=1,
        gpu_memory_utilization=0.4,
        # max_num_seqs=self.args.per_device_train_batch_size
        # * self.vllm_tensor_parallel_size
        # * self.args.steps_per_generation,
        max_model_len=2048,
        # distributed_executor_backend="mp",
        # Feed identical seed for tp groups to ensure sampling results are the same across workers
        # seed=self.accelerator.process_index // self.vllm_tensor_parallel_size,
        # Latest vLLM v1 memory profiler is misled by the high default value (i.e., 32768) - thinking there's not enough memory
        max_num_batched_tokens=4096,
        # model_impl=self.args.vllm_model_impl,
        # enable_sleep_mode=self.args.vllm_enable_sleep_mode,
        # Important so temperature scaling/logit tweaking affects the TIS log probs
        logprobs_mode="processed_logprobs",
    )

    # generate util for calling vllm
    def generate(prompts: list[str], model, temperature=0, responses_per_prompt=1):
        """
        takes in a list of prompts list[str]
        and returns a list of outputs list[str]
        """
        # move weights from model to vllm
        weight_tuples = [(n, p) for n, p in model.named_parameters()]
        vllm_internal_model = (
            vllm_model.llm_engine.model_executor.driver_worker.model_runner.model
        )
        vllm_internal_model.load_weights(weight_tuples)

        sampling_params = SamplingParams(
            max_tokens=1024,
            temperature=temperature,
            n=responses_per_prompt,
        )

        outputs = vllm_model.generate(prompts, sampling_params)
        outputs = [o.text for output in outputs for o in output.outputs]
        return outputs

    def tokenize_prompt_and_output(
        prompt_strs: list[str],
        output_strs: list[str],
        tokenizer: PreTrainedTokenizerBase,
    ) -> dict[str, torch.Tensor]:
        """
        INPUT: a list of prompts and outputs and a tokenizer
        OUTPUT: all of shape (len(prompt_strs), max_len - 1)
            - input_ids: input tokens
            - labels: output tokens (basically the inputs shifted 1 to the left)
            - response_mask: 1 indicates token that the model generate, 0 means prompt or padding
        """
        prompt_t = [tokenizer.encode(p) for p in prompt_strs]
        output_t = [tokenizer.encode(o) for o in output_strs]
        full = []
        max_len = 0

        for i in range(len(prompt_t)):
            row_len = len(prompt_t[i]) + len(output_t[i])
            max_len = max(max_len, row_len)

        for i in range(len(prompt_t)):
            padding_size = max_len - len(prompt_t[i]) - len(output_t[i])
            padding = [tokenizer.pad_token_id] * padding_size
            row = torch.tensor(prompt_t[i] + output_t[i] + padding, dtype=torch.long)
            full.append(row.unsqueeze(0))

        f2 = torch.cat(full)
        input_ids = f2[:, :-1]
        labels = f2[:, 1:]

        response_mask = torch.zeros(len(prompt_strs), max_len - 1)
        for i in range(len(prompt_t)):
            response_mask[
                i, len(prompt_t[i]) - 1 : len(prompt_t[i]) + len(output_t[i]) - 1
            ] = 1
        return {
            "input_ids": input_ids,
            "labels": labels,
            "response_mask": response_mask.bool(),
        }

    def get_response_log_probs(
        model: torch.nn.Module,
        input_ids: torch.Tensor,
        labels: torch.Tensor,
    ) -> torch.Tensor:
        logits = model(input_ids).logits
        z = logits - logits.max(dim=-1, keepdim=True).values
        exp_z = torch.exp(z)
        denom = exp_z.sum(dim=-1, keepdim=True)
        p = exp_z / denom
        logprobs = z - torch.log(denom)
        logprobs_for_label = torch.gather(
            logprobs, dim=-1, index=labels.unsqueeze(-1)
        ).squeeze(-1)
        return logprobs_for_label

    def eval_model(model, step):
        val_prompts = val_dataset[:1000]["prompt"]

        eval_start = time.time()
        outputs = generate(val_prompts, model, temperature=0)
        eval_time = time.time() - eval_start

        correct = 0
        idx_correct = []
        idx_wrong = []

        for i in range(len(outputs)):
            correct_answer = val_dataset[i]["answer"]
            generated_answer = remove_boxed(last_boxed_only_string(outputs[i]))

            if generated_answer != None and is_equiv(generated_answer, correct_answer):
                correct += 1
                idx_correct.append(i)
            else:
                idx_wrong.append(i)

        accuracy = correct / len(outputs)
        print(f"step={step}, correct: {correct} / {len(outputs)} ({accuracy:.2%})")

        # Log to wandb
        if not args.disable_wandb:
            wandb.log(
                {
                    "eval/accuracy": accuracy,
                    "eval/correct": correct,
                    "eval/total": len(outputs),
                    "eval/time_seconds": eval_time,
                },
                step=step,
            )

        return correct, idx_correct, idx_wrong

    device = "cuda:0"

    # Load model
    model_kwargs = dict(
        attn_implementation="flash_attention_2",
        torch_dtype=torch.bfloat16,
        use_cache=False,
        device_map=device,
    )
    model = AutoModelForCausalLM.from_pretrained(args.model_id, **model_kwargs)

    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    # Training loop
    print("Starting initial evaluation...")
    model = model.to(device)
    c, ic, iw = eval_model(model, 0)
    model.train()

    for i in range(args.n_grpo_steps):
        step_start_time = time.time()

        sample_indices = random.sample(
            range(0, len(train_dataset)), args.n_prompts_per_step
        )
        batch = train_dataset[sample_indices]

        # Generate rollouts
        generation_start = time.time()
        outputs = generate(
            batch["prompt"],
            model=model,
            temperature=1,
            responses_per_prompt=args.group_size,
        )
        generation_time = time.time() - generation_start

        # Extract answers
        generated_answers = [remove_boxed(last_boxed_only_string(o)) for o in outputs]

        # Compute advantages
        raw_reward = [
            a != None and is_equiv(a, batch["answer"][i // args.group_size])
            for i, a in enumerate(generated_answers)
        ]
        raw_reward_tensor = torch.tensor(raw_reward, dtype=torch.float).reshape(
            (args.n_prompts_per_step, args.group_size)
        )
        means = raw_reward_tensor.mean(dim=-1).unsqueeze(1)
        advantages = (raw_reward_tensor - means).reshape(
            (args.n_prompts_per_step * args.group_size,)
        )

        # Reward statistics
        train_accuracy = raw_reward_tensor.mean().item()
        reward_std = raw_reward_tensor.std().item()
        reward_max = raw_reward_tensor.max().item()
        reward_min = raw_reward_tensor.min().item()
        advantage_mean = advantages.mean().item()
        advantage_std = advantages.std().item()
        advantage_max = advantages.max().item()
        advantage_min = advantages.min().item()

        # Tokenize
        prompts_expanded = [x for x in batch["prompt"] for _ in range(args.group_size)]
        data = tokenize_prompt_and_output(prompts_expanded, outputs, tokenizer)
        input_ids = data["input_ids"].to(device)
        labels = data["labels"].to(device)
        response_mask = data["response_mask"].to(device)

        # Compute old log probs
        with torch.inference_mode():
            old_logprobs_all = []
            for b in range(len(input_ids) // args.micro_batch_size):
                idx = b * args.micro_batch_size
                end = idx + args.micro_batch_size
                x = input_ids[idx:end]
                y = labels[idx:end]
                old_logprobs_all.append(get_response_log_probs(model, x, y).detach())
            old_logprobs_all = torch.cat(old_logprobs_all, dim=0)
            assert old_logprobs_all.shape == labels.shape

        # Train for epochs_per_step
        epoch_grad_norms = []
        epoch_policy_ratios = []
        epoch_kl_divs = []

        training_start = time.time()
        for epoch in range(args.epochs_per_step):
            for b in tqdm(
                range(len(input_ids) // args.micro_batch_size),
                desc=f"Step {i + 1}/{args.n_grpo_steps}, Epoch {epoch + 1}/{args.epochs_per_step}",
            ):
                idx = b * args.micro_batch_size
                end = idx + args.micro_batch_size
                x = input_ids[idx:end]
                y = labels[idx:end]
                mask = response_mask[idx:end]
                micro_batch_adv = advantages[idx:end].unsqueeze(-1).to(device)

                # Get the per token log probs of each rollout
                policy_logprobs = get_response_log_probs(model, x, y)
                old_logprobs = old_logprobs_all[idx:end]

                # Compute per token loss
                ratio = torch.exp(policy_logprobs - old_logprobs)
                per_token_loss = -ratio * micro_batch_adv

                # Compute KL divergence (approximate)
                with torch.no_grad():
                    # KL(old || new) â‰ˆ (ratio - 1) - log(ratio)
                    approx_kl = ((ratio - 1) - torch.log(ratio)) * mask
                    approx_kl_mean = approx_kl.sum() / mask.sum()
                    epoch_kl_divs.append(approx_kl_mean.item())

                    # Policy ratio statistics
                    policy_ratio_mean = (ratio * mask).sum() / mask.sum()
                    epoch_policy_ratios.append(policy_ratio_mean.item())

                # Apply mask so we only train on the generated tokens
                masked_loss = per_token_loss * mask

                # Compute a scalar loss and call backward()
                denom = mask.sum(dim=-1).clamp_min(1)
                loss_per_prompt = masked_loss.sum(dim=-1) / denom
                loss = loss_per_prompt.mean() / args.gradient_accumulation_steps

                loss.backward()

                if (b + 1) % args.gradient_accumulation_steps == 0:
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    epoch_grad_norms.append(grad_norm.item())
                    optimizer.step()
                    optimizer.zero_grad()

        training_time = time.time() - training_start
        step_time = time.time() - step_start_time

        # Compute statistics
        mean_grad_norm = (
            sum(epoch_grad_norms) / len(epoch_grad_norms) if epoch_grad_norms else 0
        )
        mean_policy_ratio = (
            sum(epoch_policy_ratios) / len(epoch_policy_ratios)
            if epoch_policy_ratios
            else 0
        )
        mean_kl = sum(epoch_kl_divs) / len(epoch_kl_divs) if epoch_kl_divs else 0

        # Count total tokens generated
        total_tokens = response_mask.sum().item()
        tokens_per_sec = total_tokens / generation_time if generation_time > 0 else 0
        avg_generation_len = int(response_mask.sum(dim=-1).float().mean().item())

        # Log metrics to wandb
        if not args.disable_wandb:
            wandb.log(
                {
                    # Reward metrics
                    "train/accuracy": train_accuracy,
                    "train/reward_mean": train_accuracy,
                    "train/reward_std": reward_std,
                    "train/reward_max": reward_max,
                    "train/reward_min": reward_min,
                    # Advantage metrics
                    "train/advantage_mean": advantage_mean,
                    "train/advantage_std": advantage_std,
                    "train/advantage_max": advantage_max,
                    "train/advantage_min": advantage_min,
                    # Training dynamics
                    "train/grad_norm": mean_grad_norm,
                    "train/policy_ratio": mean_policy_ratio,
                    "train/approx_kl": mean_kl,
                    "train/avg_gen_length": avg_generation_len,
                    # Timing metrics
                    "time/step_time": step_time,
                    "time/generation_time": generation_time,
                    "time/training_time": training_time,
                    "time/tokens_per_sec": tokens_per_sec,
                },
                step=i + 1,
            )

        print(
            f"Step {i + 1}/{args.n_grpo_steps} | Train Acc: {train_accuracy:.2%} | KL: {mean_kl:.4f} | Time: {step_time:.1f}s"
        )

        if (i + 1) % 5 == 0:
            eval_model(model, i + 1)

    if not args.disable_wandb:
        wandb.finish()


if __name__ == "__main__":
    main()
