{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "import torch\n",
    "\n",
    "from main import init_model, init_training, create_connections_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, _ = create_connections_datasets(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_objects = init_training(model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-04 13:11:18.569\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgrpo\u001b[0m:\u001b[36mrollout\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mGenerating responses for 1 prompts, max_gen_len=100\u001b[0m\n",
      "\u001b[32m2025-05-04 13:11:39.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgrpo\u001b[0m:\u001b[36mrollout\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mGenerated token ids: 100\u001b[0m\n",
      "\u001b[32m2025-05-04 13:11:39.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgrpo\u001b[0m:\u001b[36mrollout\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mGenerated token ids after removing padding: 100\u001b[0m\n",
      "\u001b[32m2025-05-04 13:11:39.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgrpo\u001b[0m:\u001b[36mrollout\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mGenerated text: \n",
      "\n",
      "Okay, let's see. The user provided a list of words: pinch, rob, steal, swipe, sen, sight, sine, sour, dogleg, hairpin, switchback, zag, infant, pub, swimming, ticker. They want me to group them into four groups of four words each, using each word only once per group. \n",
      "\n",
      "First, I need to look for obvious connections. Let's start with \"sine\" and \"sight\" because\u001b[0m\n",
      "\u001b[32m2025-05-04 13:11:39.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mUpdating policy for step 1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from main import init_training\n",
    "from grpo import rollout, update_policy\n",
    "\n",
    "from tasks.countdown import reward_function\n",
    "\n",
    "to = init_training(model, train_dataset)\n",
    "batch = next(iter(to.train_dataloader))\n",
    "\n",
    "episodes = rollout(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    batch=batch,\n",
    "    max_gen_len=to.config.max_gen_len,\n",
    "    num_answer_per_question=to.config.num_answer_per_question,\n",
    "    reward_function=reward_function,\n",
    "    device=to.device,\n",
    ")\n",
    "if to.config.skip_unfinished_episodes:\n",
    "    episodes = [episode for episode in episodes if episode.is_finished]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes[0].is_finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-04 13:14:56.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgrpo\u001b[0m:\u001b[36mupdate_policy\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mUpdating policy with 1 episodes, 100 target tokens, 1 micro-batches\u001b[0m\n",
      "* Computing policy gradient:  0/ 1\u001b[0mmINFO    \u001b[0m | \u001b[36mgrpo\u001b[0m:\u001b[36mupdate_policy\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[151644, 8948, 271, 2610, 525, 458, 6203, 24626, 21828, 1614, 624, 9885, 5203, 315, 4244, 429, 525, 5435, 311, 1817, 1008, 13, 8886, 1874, 374, 3040, 4244, 1293, 13, 2619, 525, 6896, 3040, 5203, 304, 2790, 624, 2610, 1231, 1172, 990, 1817, 3409, 304, 825, 1874, 624, 65354, 304, 279, 2701, 3561, 510, 27, 19895, 287, 397, 9338, 522, 19895, 287, 397, 27, 9217, 397, 27, 4074, 397, 9338, 522, 4074, 397, 27, 4074, 397, 9338, 522, 4074, 397, 522, 9217, 397, 2, 13383, 271, 1474, 25, 37799, 11, 88916, 263, 11, 25744, 21873, 11, 25349, 11, 9055, 11, 27291, 11, 700, 11, 2874, 11, 595, 7066, 642, 11, 47332, 724, 11, 11174, 90512, 11, 40659, 388, 11, 74982, 11, 282, 524, 11, 296, 7417, 11, 259, 32779, 198, 71703, 25, 366, 19895, 287, 397, 40, 3278, 1191, 448, 14719, 1495, 279, 32711, 13, 1752, 5737, 220, 16, 11, 279, 4244, 5435, 311, 36023, 25, 37799, 11, 88916, 263, 11, 25744, 21873, 11, 25349, 1959, 20913, 311, 36023, 304, 5257, 7586, 11, 1741, 438, 88916, 263, 323, 51205, 1660, 36023, 5980, 624, 2461, 5737, 220, 17, 11, 330, 5050, 39722, 20093, 1, 33061, 311, 23261, 44291, 595, 7066, 642, 11, 47332, 724, 11, 11174, 90512, 11, 40659, 388, 624, 2808, 220, 18, 35616, 311, 17832, 11, 18202, 74982, 11, 282, 524, 11, 296, 7417, 11, 259, 32779, 13, 5737, 220, 19, 17601, 330, 2152, 1, 5435, 31747, 1075, 330, 2152, 27291, 1335, 330, 2152, 9055, 10040, 522, 19895, 287, 397, 27, 9217, 397, 27, 4074, 29, 37799, 11, 88916, 263, 11, 25744, 21873, 11, 25349, 522, 4074, 397, 27, 4074, 29, 595, 7066, 642, 11, 47332, 724, 11, 11174, 90512, 11, 40659, 388, 522, 4074, 397, 27, 4074, 29, 74982, 11, 282, 524, 11, 296, 7417, 11, 259, 32779, 522, 4074, 397, 27, 4074, 29, 9055, 11, 27291, 11, 700, 11, 2874, 522, 4074, 397, 522, 9217, 1339, 151645, 198, 151644, 872, 198, 13273, 331, 11, 10550, 11, 26571, 11, 38520, 11, 6124, 11, 13929, 11, 57668, 11, 20282, 11, 5562, 1937, 11, 6869, 13273, 11, 3398, 1419, 11, 89019, 11, 30283, 11, 6675, 11, 23380, 11, 46987, 151645, 198, 151644, 198, 151644, 198, 32313, 11, 1077, 594, 1490, 13, 576, 1196, 3897, 264, 1140, 315, 4244, 25, 49246, 11, 10550, 11, 26571, 11, 38520, 11, 6124, 11, 13929, 11, 57668, 11, 20282, 11, 5562, 1937, 11, 6869, 13273, 11, 3398, 1419, 11, 89019, 11, 30283, 11, 6675, 11, 23380, 11, 46987, 13, 2379, 1366, 752, 311, 1874, 1105, 1119, 3040, 5203, 315, 3040, 4244, 1817, 11, 1667, 1817, 3409, 1172, 3055, 817, 1874, 13, 4710, 5338, 11, 358, 1184, 311, 1401, 369, 8036, 13234, 13, 6771, 594, 1191, 448, 330, 82, 482, 1, 323, 330, 82, 491, 1, 1576]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "tensor([[151644,   8948,    271,   2610,    525,    458,   6203,  24626,  21828,\n",
      "           1614,    624,   9885,   5203,    315,   4244,    429,    525,   5435,\n",
      "            311,   1817,   1008,     13,   8886,   1874,    374,   3040,   4244,\n",
      "           1293,     13,   2619,    525,   6896,   3040,   5203,    304,   2790,\n",
      "            624,   2610,   1231,   1172,    990,   1817,   3409,    304,    825,\n",
      "           1874,    624,  65354,    304,    279,   2701,   3561,    510,     27,\n",
      "          19895,    287,    397,   9338,    522,  19895,    287,    397,     27,\n",
      "           9217,    397,     27,   4074,    397,   9338,    522,   4074,    397,\n",
      "             27,   4074,    397,   9338,    522,   4074,    397,    522,   9217,\n",
      "            397,      2,  13383,    271,   1474,     25,  37799,     11,  88916,\n",
      "            263,     11,  25744,  21873,     11,  25349,     11,   9055,     11,\n",
      "          27291,     11,    700,     11,   2874,     11,    595,   7066,    642,\n",
      "             11,  47332,    724,     11,  11174,  90512,     11,  40659,    388,\n",
      "             11,  74982,     11,    282,    524,     11,    296,   7417,     11,\n",
      "            259,  32779,    198,  71703,     25,    366,  19895,    287,    397,\n",
      "             40,   3278,   1191,    448,  14719,   1495,    279,  32711,     13,\n",
      "           1752,   5737,    220,     16,     11,    279,   4244,   5435,    311,\n",
      "          36023,     25,  37799,     11,  88916,    263,     11,  25744,  21873,\n",
      "             11,  25349,   1959,  20913,    311,  36023,    304,   5257,   7586,\n",
      "             11,   1741,    438,  88916,    263,    323,  51205,   1660,  36023,\n",
      "           5980,    624,   2461,   5737,    220,     17,     11,    330,   5050,\n",
      "          39722,  20093,      1,  33061,    311,  23261,  44291,    595,   7066,\n",
      "            642,     11,  47332,    724,     11,  11174,  90512,     11,  40659,\n",
      "            388,    624,   2808,    220,     18,  35616,    311,  17832,     11,\n",
      "          18202,  74982,     11,    282,    524,     11,    296,   7417,     11,\n",
      "            259,  32779,     13,   5737,    220,     19,  17601,    330,   2152,\n",
      "              1,   5435,  31747,   1075,    330,   2152,  27291,   1335,    330,\n",
      "           2152,   9055,  10040,    522,  19895,    287,    397,     27,   9217,\n",
      "            397,     27,   4074,     29,  37799,     11,  88916,    263,     11,\n",
      "          25744,  21873,     11,  25349,    522,   4074,    397,     27,   4074,\n",
      "             29,    595,   7066,    642,     11,  47332,    724,     11,  11174,\n",
      "          90512,     11,  40659,    388,    522,   4074,    397,     27,   4074,\n",
      "             29,  74982,     11,    282,    524,     11,    296,   7417,     11,\n",
      "            259,  32779,    522,   4074,    397,     27,   4074,     29,   9055,\n",
      "             11,  27291,     11,    700,     11,   2874,    522,   4074,    397,\n",
      "            522,   9217,   1339, 151645,    198, 151644,    872,    198,  13273,\n",
      "            331,     11,  10550,     11,  26571,     11,  38520,     11,   6124,\n",
      "             11,  13929,     11,  57668,     11,  20282,     11,   5562,   1937,\n",
      "             11,   6869,  13273,     11,   3398,   1419,     11,  89019,     11,\n",
      "          30283,     11,   6675,     11,  23380,     11,  46987, 151645,    198,\n",
      "         151644,    198, 151644,    198,  32313,     11,   1077,    594,   1490,\n",
      "             13,    576,   1196,   3897,    264,   1140,    315,   4244,     25,\n",
      "          49246,     11,  10550,     11,  26571,     11,  38520,     11,   6124,\n",
      "             11,  13929,     11,  57668,     11,  20282,     11,   5562,   1937,\n",
      "             11,   6869,  13273,     11,   3398,   1419,     11,  89019,     11,\n",
      "          30283,     11,   6675,     11,  23380,     11,  46987,     13,   2379,\n",
      "           1366,    752,    311,   1874,   1105,   1119,   3040,   5203,    315,\n",
      "           3040,   4244,   1817,     11,   1667,   1817,   3409,   1172,   3055,\n",
      "            817,   1874,     13,   4710,   5338,     11,    358,   1184,    311,\n",
      "           1401,    369,   8036,  13234,     13,   6771,    594,   1191,    448,\n",
      "            330,     82,    482,      1,    323,    330,     82,    491,      1]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Update policy - compute loss and perform backward pass\n",
    "results = update_policy(\n",
    "    model=model,\n",
    "    optimizer=to.optimizer,\n",
    "    episodes=episodes,\n",
    "    micro_batch_size=to.config.micro_batch_size,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    max_grad_norm=to.config.max_grad_norm,\n",
    "    device=to.device,\n",
    "    dtype=to.dtype,\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': nan, 'grad_norm': nan, 'entropy': 0.31090065836906433}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
