{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianfitzgerald/Documents/GitHub/minRL/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "\u001b[32m2025-04-30 19:43:33.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36minit_model\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mModel loaded.\u001b[0m\n",
      "\u001b[32m2025-04-30 19:43:33.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36minit_model\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mUsing device mps\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from main import init_model, training_loop\n",
    "import pandas as pd\n",
    "from dataset import create_connections_datasets\n",
    "from main import get_available_device\n",
    "\n",
    "tokenizer, model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, _ = create_connections_datasets(tokenizer)\n",
    "device = get_available_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-30 19:45:43.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mtraining_loop\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mStarting training loop\u001b[0m\n",
      "\u001b[32m2025-04-30 19:45:43.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mtraining_loop\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mTraining loop initialized\u001b[0m\n",
      "\u001b[32m2025-04-30 19:45:43.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mtraining_loop\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mStarting rollout for step 1\u001b[0m\n",
      "\u001b[32m2025-04-30 19:45:43.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgrpo\u001b[0m:\u001b[36mrollout\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mGenerating responses for 1 prompts, max_gen_len=100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch pref 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-30 19:45:48.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgrpo\u001b[0m:\u001b[36mrollout\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mGenerated token ids: 100\u001b[0m\n",
      "\u001b[32m2025-04-30 19:45:48.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgrpo\u001b[0m:\u001b[36mrollout\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mGenerated token ids after removing padding: 100\u001b[0m\n",
      "\u001b[32m2025-04-30 19:45:48.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgrpo\u001b[0m:\u001b[36mrollout\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mGenerated text: \n",
      "user\n",
      "pocket, key, lock, button, door, drawer, handle, hinge, compartment, container, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot,\u001b[0m\n",
      "\u001b[32m2025-04-30 19:45:48.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmain\u001b[0m:\u001b[36mtraining_loop\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mUpdating policy for step 1\u001b[0m\n",
      "\u001b[32m2025-04-30 19:45:48.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgrpo\u001b[0m:\u001b[36mupdate_policy\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mUpdating policy with 0 episodes, 0 target tokens, 0 micro-batches\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode(prefix='<|im_start|>system\\n\\nYou are an expert puzzle solving model.\\nFind groups of words that are related to each other. Each group is four words long. There are exactly four groups in total.\\nYou may only use each word in one group.\\nRespond in the following format:\\n<reasoning>\\n...\\n</reasoning>\\n<answer>\\n<group>\\n...\\n</group>\\n<group>\\n...\\n</group>\\n</answer>\\n# Example\\n\\nUser: candle, crayon, honeycomb, seal, defense, excuse, out, reason, kettles, mittens, raindrops, whiskers, canine, fang, molar, tusk\\nAssistant: <reasoning>\\nI\\'ll start with breaking down the reasoning. For Group 1, the words related to wax: candle, crayon, honeycomb, seal — connecting to wax in various forms, such as crayon and candles being wax-based.\\nFor Group 2, \"My Favorite Things\" connects to lyrics mentioning kettles, mittens, raindrops, whiskers.\\nGroup 3 relates to teeth, covering canine, fang, molar, tusk. Group 4 involves \"no\" related phrases like \"no excuse,\" \"no defense.\"\\n</reasoning>\\n<answer>\\n<group> candle, crayon, honeycomb, seal</group>\\n<group> kettles, mittens, raindrops, whiskers</group>\\n<group> canine, fang, molar, tusk</group>\\n<group> defense, excuse, out, reason</group>\\n</answer>\\n\\n<|im_end|>\\n<|im_start|>user\\npole, rod, staff, stick, acoustic, auditory, heard, sonic, balcony, box, orchestra, stage, follow, like, share, subscribe<|im_end|>\\n', text='<|im_start|>system\\n\\nYou are an expert puzzle solving model.\\nFind groups of words that are related to each other. Each group is four words long. There are exactly four groups in total.\\nYou may only use each word in one group.\\nRespond in the following format:\\n<reasoning>\\n...\\n</reasoning>\\n<answer>\\n<group>\\n...\\n</group>\\n<group>\\n...\\n</group>\\n</answer>\\n# Example\\n\\nUser: candle, crayon, honeycomb, seal, defense, excuse, out, reason, kettles, mittens, raindrops, whiskers, canine, fang, molar, tusk\\nAssistant: <reasoning>\\nI\\'ll start with breaking down the reasoning. For Group 1, the words related to wax: candle, crayon, honeycomb, seal — connecting to wax in various forms, such as crayon and candles being wax-based.\\nFor Group 2, \"My Favorite Things\" connects to lyrics mentioning kettles, mittens, raindrops, whiskers.\\nGroup 3 relates to teeth, covering canine, fang, molar, tusk. Group 4 involves \"no\" related phrases like \"no excuse,\" \"no defense.\"\\n</reasoning>\\n<answer>\\n<group> candle, crayon, honeycomb, seal</group>\\n<group> kettles, mittens, raindrops, whiskers</group>\\n<group> canine, fang, molar, tusk</group>\\n<group> defense, excuse, out, reason</group>\\n</answer>\\n\\n<|im_end|>\\n<|im_start|>user\\npole, rod, staff, stick, acoustic, auditory, heard, sonic, balcony, box, orchestra, stage, follow, like, share, subscribe<|im_end|>\\n\\nuser\\npocket, key, lock, button, door, drawer, handle, hinge, compartment, container, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot, slot,', prefix_token_ids=[151644, 8948, 271, 2610, 525, 458, 6203, 24626, 21828, 1614, 624, 9885, 5203, 315, 4244, 429, 525, 5435, 311, 1817, 1008, 13, 8886, 1874, 374, 3040, 4244, 1293, 13, 2619, 525, 6896, 3040, 5203, 304, 2790, 624, 2610, 1231, 1172, 990, 1817, 3409, 304, 825, 1874, 624, 65354, 304, 279, 2701, 3561, 510, 27, 19895, 287, 397, 9338, 522, 19895, 287, 397, 27, 9217, 397, 27, 4074, 397, 9338, 522, 4074, 397, 27, 4074, 397, 9338, 522, 4074, 397, 522, 9217, 397, 2, 13383, 271, 1474, 25, 37799, 11, 88916, 263, 11, 25744, 21873, 11, 25349, 11, 9055, 11, 27291, 11, 700, 11, 2874, 11, 595, 7066, 642, 11, 47332, 724, 11, 11174, 90512, 11, 40659, 388, 11, 74982, 11, 282, 524, 11, 296, 7417, 11, 259, 32779, 198, 71703, 25, 366, 19895, 287, 397, 40, 3278, 1191, 448, 14719, 1495, 279, 32711, 13, 1752, 5737, 220, 16, 11, 279, 4244, 5435, 311, 36023, 25, 37799, 11, 88916, 263, 11, 25744, 21873, 11, 25349, 1959, 20913, 311, 36023, 304, 5257, 7586, 11, 1741, 438, 88916, 263, 323, 51205, 1660, 36023, 5980, 624, 2461, 5737, 220, 17, 11, 330, 5050, 39722, 20093, 1, 33061, 311, 23261, 44291, 595, 7066, 642, 11, 47332, 724, 11, 11174, 90512, 11, 40659, 388, 624, 2808, 220, 18, 35616, 311, 17832, 11, 18202, 74982, 11, 282, 524, 11, 296, 7417, 11, 259, 32779, 13, 5737, 220, 19, 17601, 330, 2152, 1, 5435, 31747, 1075, 330, 2152, 27291, 1335, 330, 2152, 9055, 10040, 522, 19895, 287, 397, 27, 9217, 397, 27, 4074, 29, 37799, 11, 88916, 263, 11, 25744, 21873, 11, 25349, 522, 4074, 397, 27, 4074, 29, 595, 7066, 642, 11, 47332, 724, 11, 11174, 90512, 11, 40659, 388, 522, 4074, 397, 27, 4074, 29, 74982, 11, 282, 524, 11, 296, 7417, 11, 259, 32779, 522, 4074, 397, 27, 4074, 29, 9055, 11, 27291, 11, 700, 11, 2874, 522, 4074, 397, 522, 9217, 1339, 151645, 198, 151644, 872, 198, 68072, 11, 20589, 11, 5570, 11, 9214, 11, 44066, 11, 82529, 11, 6617, 11, 71336, 11, 43852, 11, 3745, 11, 68055, 11, 6430, 11, 1795, 11, 1075, 11, 4332, 11, 17963, 151645, 198], generated_token_ids=[151644, 198, 151644, 872, 198, 79, 4537, 11, 1376, 11, 5296, 11, 3137, 11, 6006, 11, 26482, 11, 3705, 11, 75593, 11, 43498, 11, 5476, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11, 9446, 11], is_finished=False, reward=0.0, reward_info={'format_reward': 0.0, 'answer_reward': 0.0})]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'loss' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:20\u001b[39m, in \u001b[36mtraining_loop\u001b[39m\u001b[34m(model, tokenizer, train_dataset, device)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/minRL/grpo.py:232\u001b[39m, in \u001b[36mupdate_policy\u001b[39m\u001b[34m(model, optimizer, episodes, micro_batch_size, pad_token_id, max_grad_norm, device, dtype)\u001b[39m\n\u001b[32m    229\u001b[39m optimizer.step()\n\u001b[32m    230\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mloss\u001b[49m.item(),\n\u001b[32m    233\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgrad_norm\u001b[39m\u001b[33m\"\u001b[39m: grad_norm.item(),\n\u001b[32m    234\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mentropy\u001b[39m\u001b[33m\"\u001b[39m: entropy.item(),\n\u001b[32m    235\u001b[39m }\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'loss' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "training_loop(model, tokenizer, train_dataset, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
