{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brianf/minRL/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "import torch\n",
    "\n",
    "from train import init_model, init_training, create_connections_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-09 19:24:54.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrain\u001b[0m:\u001b[36minit_model\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mModel loaded.\u001b[0m\n",
      "\u001b[32m2025-05-09 19:24:54.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrain\u001b[0m:\u001b[36minit_model\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mUsing device cuda:0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, _ = create_connections_datasets(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_objects = init_training(model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-09 19:25:02.502\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgrpo\u001b[0m:\u001b[36mrollout\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mGenerating responses for 2 prompts, max_tokens=256\u001b[0m\n",
      "\u001b[32m2025-05-09 19:26:05.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgrpo\u001b[0m:\u001b[36mrollout\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerated token ids: 256\u001b[0m\n",
      "\u001b[32m2025-05-09 19:26:05.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgrpo\u001b[0m:\u001b[36mrollout\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mGenerated token ids after removing padding: 256\u001b[0m\n",
      "\u001b[32m2025-05-09 19:26:05.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgrpo\u001b[0m:\u001b[36mrollout\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mGenerated text: \n",
      "\n",
      "Okay, let's see. The user provided a list of words: bottom, candy, garden, star, boo, honey, sugar, sweetie, arizona, colorado, nevada, utah, comb, hive, honey, wax. They want me to form four groups of four words each, with each word used only once per group. \n",
      "\n",
      "First, I need to look for words that can form a group. Let's start with the first group. The words \"bottom,\" \"candy,\" \"garden,\" and \"star\" all seem to be related to nature. \"Bottom\" could be part of a place, \"candy\" and \"honey\" as sweet things, \"garden\" and \"star\" as natural elements. That makes sense. So Group 1: bottom, candy, garden, star.\n",
      "\n",
      "Next, looking for another group. \"Honey,\" \"honey,\" \"comb,\" and \"hive\" are both related to honey and hive. So Group 2: honey, honey, comb, hive. Wait, but each word can only be in one group. Oh, right, there are duplicates here. So maybe \"honey\" and \"honey\" aren't allowed.\u001b[0m\n",
      "\u001b[32m2025-05-09 19:26:05.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgrpo\u001b[0m:\u001b[36mrollout\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerated token ids: 256\u001b[0m\n",
      "\u001b[32m2025-05-09 19:26:05.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgrpo\u001b[0m:\u001b[36mrollout\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mGenerated token ids after removing padding: 256\u001b[0m\n",
      "\u001b[32m2025-05-09 19:26:05.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgrpo\u001b[0m:\u001b[36mrollout\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mGenerated text: \n",
      "\n",
      "Okay, let's see. The user provided a list of words: bottom, candy, garden, star, boo, honey, sugar, sweetie, arizona, colorado, nevada, utah, comb, hive, honey, wax. They want me to group them into four groups of four words each, using each word only once per group. \n",
      "\n",
      "First, I need to look for possible connections. Let's start with the first group. Words like \"bottom,\" \"garden,\" \"star,\" and \"honey\" â€“ maybe \"bottom\" and \"garden\" are related to nature, \"star\" and \"honey\" could be related to something sweet. But \"star\" and \"honey\" don't seem directly related. Wait, \"star\" and \"honey\" could be in a different context. Maybe \"star\" is part of a phrase like \"starlight,\" but I don't see a direct connection. Let me check again.\n",
      "\n",
      "Looking at the list again: bottom, candy, garden, star, boo, honey, sugar, sweetie, arizona, colorado, nevada, utah, comb, hive, honey, wax. \n",
      "\n",
      "Hmm. \"Comb\" and \"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards {'reward': 0.0, 'reward_info': {'format_reward': 0.0, 'answer_reward': 0.0}}\n",
      "rewards {'reward': 0.0, 'reward_info': {'format_reward': 0.0, 'answer_reward': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "from train import init_training\n",
    "from grpo import rollout, update_policy\n",
    "\n",
    "from tasks.countdown import reward_function\n",
    "\n",
    "to = init_training(model, train_dataset)\n",
    "batch = next(iter(to.train_dataloader))\n",
    "\n",
    "episodes = rollout(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    batch=batch,\n",
    "    max_new_tokens=to.config.max_new_tokens,\n",
    "    num_answer_per_question=to.config.num_answer_per_question,\n",
    "    reward_function=reward_function,\n",
    "    device=to.device,\n",
    ")\n",
    "if to.config.skip_unfinished_episodes:\n",
    "    episodes = [episode for episode in episodes if episode.is_finished]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes[0].reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodes [0.0, 0.0]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Update policy - compute loss and perform backward pass\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mupdate_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapply_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/minRL/grpo.py:172\u001b[39m, in \u001b[36mupdate_policy\u001b[39m\u001b[34m(model, optimizer, episodes, micro_batch_size, pad_token_id, max_grad_norm, device, dtype, apply_loss)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_policy\u001b[39m(\n\u001b[32m    161\u001b[39m     model: nn.Module,\n\u001b[32m    162\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    169\u001b[39m     apply_loss: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    170\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    171\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mepisodes\u001b[39m\u001b[33m\"\u001b[39m, [episode.reward \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m episodes])\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     episodes = \u001b[43mnormalize_rewards_per_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# sort episodes by length, for more efficient batching\u001b[39;00m\n\u001b[32m    174\u001b[39m     episodes.sort(\n\u001b[32m    175\u001b[39m         key=\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x.prefix_token_ids) + \u001b[38;5;28mlen\u001b[39m(x.generated_token_ids), reverse=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    176\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/minRL/grpo.py:137\u001b[39m, in \u001b[36mnormalize_rewards_per_group\u001b[39m\u001b[34m(episodes)\u001b[39m\n\u001b[32m    135\u001b[39m normalized_episodes: List[Episode] = []\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m groups.values():\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     rewards = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m     mean = rewards.mean()\n\u001b[32m    139\u001b[39m     std = rewards.std()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/minRL/.venv/lib/python3.12/site-packages/torch/utils/_device.py:104\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    103\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Update policy - compute loss and perform backward pass\n",
    "results = update_policy(\n",
    "    model=model,\n",
    "    optimizer=to.optimizer,\n",
    "    episodes=episodes,\n",
    "    micro_batch_size=to.config.micro_batch_size,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    max_grad_norm=to.config.max_grad_norm,\n",
    "    device=to.device,\n",
    "    dtype=to.dtype,\n",
    "    apply_loss=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
